## Who am I?
![](figures/dcg.png)

::: notes
Just a brief presentation, I am an associate professor and researcher in the field of distributed computing at a small university in Spain. I am a member of the Distributed Computing Group at the University of Lleida, a city located about two hours from Barcelona. As you can see from our corporate slide, our group is small.
:::

## Introduction {.smaller}

![](figures/evolution.png)


::: {.fragment style="text-align: center;"}
### Data Centers
:::

::: notes
Sam asked me to provide an overview of data centers from a computational perspective. I won't be presenting my own work, but rather discussing the evolution and scale of data centers and the challenges they present in this era. Let's start by taking a quick look at the recent history of technology. Our journey began with the Internet era, connecting us ...and thanks to virtualization, we moved into the era of cloud computing, enabling massive scale, but technology didn't stop there. We're now seeing a shift towards a cloud continuum where edge devices, fog servers, and the central cloud coexist and collaborate. And most recently, we've seen the explosive rise of AI.  But what is the silent, unseen foundation that enables this evolution? $\downarrow$ The answer is Data Centers.
:::

## What is this talk about... {.smaller}

#### Modern data centers must manage *diverse workloads*, each with *unique demands* on shared, *heterogeneous hardware*. Moreover, modern data centers are often *geo-distributed*, forming a network of multiple facilities rather than a single monolithic center.

:::::: columns
::: {.column width="35%"}
![Overview of the talk purpose. *Image generated by the author using AI (Gemini 2.5 Flash)*.](figures/task_management.png)
:::

:::: {.column width="65%"}
1.  **Workload diversity**: Different workloads have distinct *personalities* and *resource requirements*\
2.  **Heterogeneous hardware**: CPUs, GPUs, TPUs, NICs
3.  **Multi-datacenter coordination**: Geo-distributed load balancing
4.  **Quality of Service (QoS)**: Ensuring SLA criteria are met for all workloads and services types.
5.  **Resilience**: Maintaining service availability in the face of failures or unexpected changes.

::: {.fragment style="text-align: center"}
#### Understanding these challenges and exploring current solutions is the key focus of this talk.
:::
::::
::::::

::: notes
This talk is all about the complexity of modern data centers. $\downarrow$ We have workload diversity. A data center might be running a real-time video game alongside a massive AI training job. These different applications have unique **personalities** and resource demands. $\downarrow$ It's not just CPUs anymore. We've got GPUs, TPUs, ... $\downarrow$ Our systems are no longer a single, monolithic building. They are geo-distributed, forming a network of multiple facilities. This creates new challenges for coordination, like balancing loads across different continents while dealing with data locality and latency. $\downarrow$  How do we ensure that a user's critical transaction gets priority over a background analysis job? $\downarrow$ In a world of constant change and potential failures, how do we maintain service availability and ensure nothing goes down? $\downarrow$ So, the key focus of this talk is to understand these challenges and explore some of the solutions.
:::



## What is a workload? {.smaller}

A **workload** is the total demand that processes and users place on a computing system over a specific period.

1.  **Batch Processing**: Large volumes of data processed in chunks (e.g., ETL jobs).
2.  **Interactive Processing**: Real-time user interactions (e.g., web applications).
3.  **Stream Processing**: Continuous data streams processed in real-time (e.g., IoT data).
4.  **AI/ML Workloads**: Resource-intensive training and inference tasks (e.g., neural networks).
5.  **High-Performance Computing (HPC)**: Compute-intensive simulations and modeling (e.g., scientific research).

::: fragment
#### A video streaming service (*Netflix*) is an application while the workload is the varying demand of that service as thousands of users simultaneously stream videos at different resolutions.
:::

::: notes
It's easy to confuse a workload with a simple application. The key is to remember that a workload is dynamic—it’s the total demand a system has, not just one app.  We can categorize them into different types. $\downarrow$ Batch Processing is for heavy but scheduled tasks, like a financial report at the end of the day. $\downarrow$ Then we have Interactive Processing, which handles real-time user requests, like a social media app. Stream Processing is similar, but it deals with a continuous flow of data, like from a sensor. $\downarrow$ A new and important type is AI/ML workloads, which are very hungry for resources, especially GPUs.$\downarrow$ Finally, there's High-Performance Computing, which has been around for decades for things like scientific simulations.$\downarrow$ To make this clear, think of Netflix. The Netflix application is the software itself. But the workload is the constantly changing demand it puts on the data center as millions of users stream shows at different resolutions.
:::


## What is a Data Center? {.smaller}

A **Data Center** is a facility that houses a large number of servers and networking equipment (**Heterogeneous resources**) to provide computing resources for various applications and services (**diverse workloads**).

![Data Center Complexity and Heterogeneity.](figures/datacenter.png)

#### It is a complex distributed system that manages the flow of data and compute resources to meet the demands of users and applications. {.center .fragment}

::: notes
So, we've talked about the different types of workloads. Now let's look at the system that handles them: the data center. At its heart, a data center is a huge building full of computers and equipment. The image shows the sheer variety of heterogeneous resources inside: different CPUs, storage drives, and networking gear. They all have different strengths and weaknesses. A data center’s main job is to intelligently match the right hardware to the right workload. It has to serve both an AI training job that needs a lot of GPU power and a database that needs fast storage—all at the same time. $\downarrow$ So, a data center is not just a building. It's a complex distributed system that constantly manages the flow of data and resources to meet all these different demands.
:::


## Cloud Computing Era

#### **Cloud computing** abstracts away the hardware, offering compute resources as a utility. Users consume services (e.g., VMs, storage, functions) and stop managing the physical infrastructure.

![](figures/pizza.png){.fragment .center}

::: {.fragment .center}
> AWS EC2 achieves **99.99% uptime** for cloud services.
:::




:::notes
For the last two decades, cloud computing has been the main way we handle data center complexity. The idea is simple: you don't manage the physical hardware anymore; you just use computing as a service, like a utility. $\downarrow$ The image here uses a pizza analogy to explain this. If you make a pizza at home from scratch, you do everything. This is like the old way of doing things—you manage all your own servers and software.  With Infrastructure as a Service, someone provides the basic parts (ingredients), and you still have to build and bake it. With Platform as a Service, they make the pizza for you and deliver it. You just need to serve it. And with Software as a Service, it's like eating at a restaurant—everything is taken care of. $\downarrow$ This approach was very successful and reliable, which is why services like AWS can promise nearly perfect uptime. The most important thing to remember is this: even in the cloud, someone still has to manage the physical servers.
:::

## Data Centers and Power Plants

::: {.fragment .center}
Both manage *complex* **distributed**, *dynamic* flows — one of electricity, one of compute/data.
:::

::: {.fragment .center}
#### Shared Challenges

1.  **Resource Management**: Heterogeneous hardware (CPUs, GPUs) vs. generators (coal, solar).
2.  **Workload Management**: Steady loads + scheduled peaks (e.g., daily spikes in web traffic vs. industrial power demand).
3.  **Spiky Demands**: AI training bursts vs. storm-driven power surges.
:::




::: notes
To understand data centers better, let’s compare them to something we all know: a power plant. $\downarrow$ They both seem different, but they are very much alike. Both systems are all about managing a complex, dynamic flow—one of electricity, the other of data. $\downarrow$  They face the same challenges. $\downarrow$ A power plant has to balance different sources like coal, solar, and wind. A data center has to do the same with its own different resources, like CPUs and GPUs. $\downarrow$ A power plant deals with daily spikes in electricity use. A data center handles daily spikes in web traffic. $\downarrow$ A sudden storm can cause a huge surge in power demand. A viral online event or a massive AI job can cause a sudden, huge spike in a data center. Both are experts at balancing supply and demand.
:::

## Beyond the infrastructure

This complexity includes not just IT equipment, but also massive **power distribution** and **cooling systems** that consume a significant portion of the total energy.

- As much as **40%** of a data center's total energy consumption can be related to cooling systems alone.
- Data center energy demands are projected to consume as much as $9%$ of US annual electricity generation by the year 2030.


::: notes

Data centers are complex because of more than just the servers, workloads, data... They also have huge power and cooling systems that use a lot of energy. $\downarrow$ 40% of a data center's total energy consumption can be related to cooling systems alone. $\downarrow$ The final point is about scale. Data center energy demands are on track to consume as much as 9% of the US's total annual electricity generation by the year 2030. This is no longer just a technical problem; it’s a global energy challenge.
:::

## Assignment Problems

Given two sets (*tasks* ↔ *resources*), find the optimal one-to-one mapping to **mix/max criterias**, under constraints.

- **Geo-distributed traffic**: Assign incoming user traffic to the most suitable data center based on latency and load.

- **Workload placement**: Assign diverse workloads to specific servers (*resource needs and current utilization*).

- **Task scheduling**: Assign individual tasks to specific CPU, GPUs based on profile and availability.

::: {.fragment style="text-align: center;"}
**Multilayer Challenge**
:::


::: notes
We've established that a data center is a lot like a power plant, balancing supply and demand. Now, let's look at the digital version of that balancing act: the assignment problem. At its core, it's about finding the best match between what we have and what we need. (*pause*) But it's not just a single problem. The complexity is compounded because it's a cascade of interconnected assignment problems happening at different layers of the infrastructure. $\downarrow$ At the highest level, you're assigning incoming user traffic to the best geo-distributed data center to minimize latency or other factors. $\downarrow$ Within that data center, you're assigning diverse workloads to specific servers and racks based on their resource needs and the server's current load. $\downarrow$ And at the lowest level, you're assigning individual tasks to specific CPU cores, GPUs, or other specialized hardware to ensure optimal performance. This multi-layered challenge.
:::

## Assignment Problems: Matter of scale 

A single large-scale data center:

::: {.fragment}
 ~100,000 servers
:::

::: {.fragment}
 ~1,000,000 CPU cores
:::

::: {.fragment}
 ~10,000 GPUs / TPUs
:::

::: {.fragment}
 ~1,000,000 concurrent tasks (heterogeneous workloads and constraints)
:::

::: {.fragment}
 ...and over 1,000,000,000 possible assignments
:::

::: {.fragment style="text-align: center;"}
#### ...and this is just one data center! How many more are out there?
:::


::: notes
Now, to fully grasp this challenge, you need to understand the scale we're talking about. $\downarrow$ A single large-scale data center can contain over 100,000 servers. $\downarrow$ That's over 1 million CPU cores and 10,000 GPUs/TPUs working together.$\downarrow$ And at any given moment, those resources are managing over 1 million concurrent, heterogeneous tasks. (*pause*). The number of possible assignments for those one million tasks is astronomical. .$\downarrow$ And this is just one data center. The problem becomes even more complex when you consider the interconnectedness of hyperscale, multi-region, and edge data centers.
:::

## Assignment Problems: Task allocation 


:::::: columns
::: {.column .fragment width="45%"}
Task CPU/GPU profile, latency tolerance, memory needs.
:::

::: {.column .fragment width="10%"}
$$\Leftrightarrow$$
:::

::: {.column .fragment width="45%"}
Generator capacity, transmission constraints.
:::
::::::


:::::: columns
::: {.column .fragment width="45%"}
Hardware limits, affinity/anti-affinity rules.
:::

::: {.column .fragment width="10%"}
$$\Leftrightarrow$$
:::

::: {.column .fragment width="45%"}
Network topology, stability margins.
:::
::::::


:::::: columns
::: {.column .fragment width="45%"}
Minimize idle cycles, Maximize throughput.
:::

::: {.column .fragment width="10%"}
$$\Leftrightarrow$$
:::

::: {.column .fragment width="45%"}
Minimize energy loss, meet demand at all buses.
:::
::::::

::: {.center .fragment}
> The problem is NP-hard, but there are techniques to find near-optimal solutions efficiently. 
:::

::: notes
We've talked about the assignment problem as a massive challenge. Now, let's look at a very specific, one that happens millions of times: task allocation. How do we assign an incoming task—like a user's web request—to the best possible resource? $\downarrow$ To do this, we first need to understand the inputs. CPU/GPU needs, memory requirements, and how much latency it can tolerate. just like a power grid operator needing to know the capacity of each power generator.$\downarrow$ Second, we have to deal with constraints. This includes hardware limits, but also things like affinity rules. These are the digital equivalent of a power grid’s network topology and stability rules. $\downarrow$ Finally, we have our objectives. For example, we want to assign tasks in a way that minimizes idle cycles and maximizes system throughput. Just like a power grid's goal of minimizing energy loss and meeting demand at all times. $\downarrow$ This is an NP-hard problem. This means finding the perfect solution is almost impossible for a system of this size. But, we have techniques to find near-optimal solutions.
:::

## Task allocation in the Cloud Era {.smaller}

::: {.fragment}
| **Technique** | **Description** | **Benefits** | **Limitations** |
|------------------|--------------------|------------------|------------------|
| **Heuristics** | Rule-based, fast decisions | Low overhead, scalable | Suboptimal solutions |
| **Approximation** | Near-optimal with performance bounds | Balances optimality, efficiency | Higher complexity |
| **Metaheuristics** | Iterative optimization for complex problems | Near-optimal solutions | Computationally intensive |
| **Constraint Programming** | Constraint satisfaction | Complex constraints | Scalability issues |
| **Machine Learning** | Data-driven, adaptive allocation | Dynamic workloads | Needs training, and high compute cost |
:::

::: notes
For a long time, we solved the task allocation problem using traditional optimization techniques. $\downarrow$ We started with simple heuristics, which are a set of hand-written rules, like "put the new task on the server with the least CPU usage." They are fast and scalable but often lead to less-than-ideal results. As things got more complex, we moved to more advanced methods. Approximation and Metaheuristics could find solutions that were close enough to the perfect one. And Constraint Programming was great for handling complex rules. These traditional approaches worked well for the predictable workloads. However, with the rise of AI and dynamic workloads, these methods started to hit their limits. Instead of us having to pre-define every rule, ML allows the system to learn the best allocation strategy directly from data.
:::


## ML Approaches to Task Allocation {.smaller}

::: fragment
#### Input (State)

*Real-time metrics on CPU/GPU usage, network traffic, task latency, memory footprint, I/O wait times, task type, deadline, dependency graph*
:::

::: fragment
#### Model (Policy / Predictor)

*Learns optimal placement via reinforcement learning (rewarding perfomance criteria, and penalizing SLA violations) or graph neural networks (learn embeddings and relationships).*
:::

::: fragment
#### Output (Decision)

-   **Predictive scheduling**: anticipate workload trends
-   **Adaptive load balancing**: live migration or scaling based on real-time metrics
-   **Proactive allocation**: pre-warm compute/storage in anticipation of spikes
:::

::: fragment
@jian2024drs achieved a \~27% increase in resource utilization and reduced load imbalance by a factor of \~2.9× compared to the default Kubernetes scheduler using deep and reinforcement learning techniques.
:::

::: notes
So, how exactly does a machine learning approach work? $\downarrow$ First, the system needs to understand the current state. Its input is real-time data on everything from CPU/GPU usage and network traffic to task deadlines. This is the data the system will learn from. $\downarrow$ Second, we have the model. This is the brain of the system. It learns the best way to place tasks by using techniques like reinforcement learning. It's rewarded when it makes a good decision—like meeting a performance goal—and penalized when it makes a bad one, like a service failure. $\downarrow$ And finally, the model generates an output—a decision. This can be a proactive or reactive action. It can predict future workload trends, adaptively balance loads in real-time, or even pre-allocate resources in anticipation of a spike. $\downarrow$ A recent research paper achieved a ~27% increase in resource utilization and a significant reduction in load imbalance compared to the default scheduler. This shows that ML-based approaches are already outperforming traditional methods.
:::

## Right-Sizing Resources: Hybrid Strategy

The paper by @chen2024learning addresses this by proposing a two-tiered system for online resource allocation, decoupling the problem to enhance scalability.

![](./figures/Hybrid%20Strategy.png)


::: notes
Now, let's look at a real-world example of how it's being used in a hybrid approach. The paper by Chen et al. proposes a two-tiered system that combines the best of both worlds: machine learning for high-level strategy and traditional optimization for low-level execution. $\downarrow$ The first tier is Strategic Planning.It doesn't decide where every single task goes. Instead, it learns the optimal order or type of servers to use for allocation. This strategic decision helps with long-term goals like ensuring fault tolerance and overall efficiency. $\downarrow$ The second tier is Tactical Execution. This is where a traditional solver takes over. Guided by the RL agent's plan, it makes the real-time, precise task placement decisions. It’s computationally intensive, but because it's only solving a small, focused part of the problem, it can quickly and efficiently map individual tasks to the best servers, making sure all the technical requirements are met.
:::

## From Algorithms to Systems

Efficient task allocation is not just about the algorithm; it's also about the underlying technologies.

::: {.fragment .center}
Orchestrator $\Rightarrow$ Workload Manager $\Rightarrow$ Execution Units
:::

- **Containers**: portable, consistent, lightweight
- **Unikernels**: single-purpose, minimal, fast


:::{.fragment}
> Orchestration now spans multi-region and global infrastructure.
:::

::: notes
We've talked about how a machine learning model can make smart decisions. But that model can't work alone. It needs a whole system to put its decisions into practice. Efficient task allocation isn't just about the algorithm; it's about the entire underlying technology stack. $\downarrow$ At the center of this is the Orchestrator. This is the brain that takes the allocation decision and manages the entire workload lifecycle—from creating and scaling a task to migrating it and tearing it down. It makes the whole data center behave like a single, unified computer. $\downarrow$ The orchestrator relies on specialized execution units to run the workloads. For a long time, these have been containers. $\downarrow$ A more recent technology is the unikernel. These are even more minimal $\downarrow$ The problem is no longer confined to a single data center. As we've seen, hyperscale providers operate across multiple regions and a globally distributed infrastructure. The orchestration challenge now extends across this entire network, which brings us to our next set of challenges.
:::


## User Perspective on Task Allocation

-   **Dev**: Expect efficient resource utilization and minimal latency for their applications. *I need tasks to auto-scale without manual tweaking.*
-   **Ops**: Require reliable performance and easy management of resources. *I need visibility into resource usage and performance metrics.*
-   **Finance**: Look for cost-effective solutions and predictable budgeting. *I need to optimize costs while meeting SLAs.*
-   **End Users**: Seek high availability and responsiveness. *Why is my app slow when the cloud is infinite?*
  
::: notes
We've discussed about the technical challenge of task allocation—the algorithms, the orchestrators, and the containers. But why does all this matter? It matters because the success of our systems is measured by their impact on real people. Task allocation is a classic problem where different people have wildly different, and often competing, expectations. $\downarrow$ For a Developer, the expectation is about efficiency and ease of use.  They need systems that can auto-scale their applications without them having to think about it. $\downarrow$ For Operations teams, it's about reliability and visibility. They need systems that are predictable and provide clear metrics to diagnose problems quickly. $\downarrow$ The Finance team has a completely different perspective. They need cost-effective solutions and predictable budgeting, so the system must be able to optimize costs without sacrificing performance. $\downarrow$ And finally, for the End User, it all comes down to a simple, fundamental expectation: high availability and responsiveness. 
:::

## Faults in DC: Meta's problem

- A snake in a power sub-station caused a *3%* capacity loss in one fault domain
- Critical services on affected servers triggered user-visible errors
- Traffic was drained from the entire data center, despite 97% of capacity remaining

::: {.fragment .center}
> How can we tolerate sub-data center faults without draining traffic?
:::

::: notes
Let's conclude this section by looking at a real-world example of the assignment problem in action from Meta. A few years ago, a very small and localized issue $\downarrow$  a snake in a power sub-station—caused a 3% capacity loss in one of their data center's fault domains. This tiny failure, however, had a massive consequence. $\downarrow$  Because critical services were concentrated in that one area, the failure triggered user-visible errors. $\downarrow$  As a response, the entire data center was drained of all traffic, despite the fact that 97% of the capacity was still perfectly functional. This overreaction highlighted a critical problem. The core question became: $\downarrow$  How can we build a system that can tolerate sub-data center faults without having to take down the entire facility?"
:::

## Faults in DC: Meta's solution

@narayanan2020fault an optimal placement strategy, data-driven strategy instead of over-provisioning:

- **Optimal Hardware Placement**: Intelligently place diverse racks (compute, storage) across fault domains to ensure no single zone is overloaded.
- **Buffer Capacity**: Reserve just enough resources—equal to one fault domain—to absorb all workloads if that zone fails.
- **Service & Data Sharding**: Distribute services and their data evenly across domains to maintain availability.


::: notes
Thus, they moved beyond simple over-provisioning and developed an intelligent, data-driven strategy. $\downarrow$ The first is Optimal Hardware Placement. They use an optimization engine to place different types of racksacross their fault domains. This ensures that no single zone becomes a critical point of failure. $\downarrow$ Second, they use a Buffer Capacity. Instead of simply having extra hardware everywhere, they reserve a specific amount of resources—the equivalent of one fault domain—to be ready to absorb workloads if a zone fails. $\downarrow$ And finally, they distribute services and their data evenly across these domains to ensure no service is ever reliant on a single point of failure. It not only finds the best place for new equipment but also continuously rebalances existing workloads and even coordinates the physical relocation of older hardware to improve the overall system balance over time.
:::


## Transition to AI Era: Demand

Based on @mckinsey2024aipower analysis:


![](figures/cloud-ai.png){style="display: block; margin:auto;"}


::: notes
So we've seen all the challenges data centers face in the cloud era. But if you think that problem is difficult, let's look at the immense pressure that AI is placing on this entire system. Based on a recent analysis from McKinsey, the demand for data center capacity is on a trajectory to explode. Global data center capacity may triple by 2030. But what's even more significant is what's driving this growth. Future data center capacity is projected to be driven by AI workloads, with Generative AI alone accounting for nearly half of that. This isn't a minor change; it's a revolution in demand.
:::

## Transition to AI Era: Expectations

Peter Pietzuch in the last IEEE ICDCS 2025, *45th IEEE International Conference on Distributed Computing Systems*

-   **Distributed AI workloads** dominate modern data centres.
-   Current AI stacks **lack adaptivity** to resource & workload changes.
-   Next-gen systems must be **adaptive by design**.

::: {.fragment .center}
> **Datacenters** must **evolve** to support these **adaptive** features, enabling more efficient resource allocation and **workload management**.
:::

::: notes
The massive demand from AI is forcing a change in how we manage data centers. The focus is no longer just on scaling up but on becoming adaptive. $\downarrow$ As Peter Pietzuch pointed out at the recent IEEE ICDCS conference, distributed AI workloads now dominate data centers. These aren't just a few large jobs; they're a constant stream of complex, interconnected tasks. $\downarrow$ The main problem is that our current AI systems lack adaptivity. They're often rigid and can't change in response to real-time resource and workload changes. This leads to wasted resources and inefficiency. $\downarrow$ Because of this, the next generation of systems must be adaptive by design. This is a major shift from static, one-size-fits-all models to dynamic and flexible. $\downarrow$ In short, our data centers must evolve to support these new features, allowing for more efficient resource allocation and workload management.
:::


## AI workload challenges

- **Training**: Resource-intensive and time-consuming.
- **Inference**: Process new data in real-time.

:::{.fragment}
![Inspired in a figure of Keynote — Peter Pietzuch (ICDCS 2025)](figures/parallelism.png){style="display: block;margin-left: auto;margin-right: auto;"}
:::

::: notes
But that's only half the story. The other half is the software and systems challenge of managing these next-generation AI workloads themselves. $\downarrow$ We have training workloads that are highly demanding in terms of resources and time, but also inference workloads  $\downarrow$  that require real-time processing of new data. What happens when they don't fit on a single GPU? It requires us to think about distribution. This has led to the development of new parallelism techniques. As this figure shows, there are three primary types of parallelism.
:::

## Data Parallelism


![](./figures/data_parallelsim.png){style="display: block; margin:auto;"}


::: {.fragment}
> Like many small generators feeding the same grid bus, all-reduce could be the frequency regulation that keeps them in sync.
:::

::: notes
Data parallelism is the most common. $\downarrow$ It's used when the entire AI model can fit into the memory of a single GPU. It works by: $\downarrow$ Replicating the model, $\downarrow$ Splitting the data, $\downarrow$ Synchronizing gradients. $\downarrow$ This is similar to a power grid where many small generators feed the same bus. The all-reduce operation acts like a frequency regulator that keeps all the generators in sync, ensuring the entire system remains stable and consistent. $\downarrow$  But what happens when the model is so large it won't even fit in a single GPU's memory? That's when we need a different approach.
:::


## Tensor Parallelism

![](./figures/tensor.png){style="display: block; margin:auto;"}

::: {.fragment}
> Like a large generator with multiple synchronized turbines $\rightarrow$ a failure in one turbine stalls the generator.
:::


::: notes
Tensor parallelism is a more complex approach used when the model itself is too large to fit on a single GPU's memory. It works by splitting the model's layers or tensors across multiple GPUs. For example, a single layer might have half of its data on one GPU and the other half on a second GPU. This requires synchronization between the GPUs, and the communication patterns are much more intricate and specialized. $\downarrow$  This is similar to a large generator with multiple synchronized turbines. If one turbine fails, the entire generator can't function.
:::

## Pipeline Parallelism

![](./figures/pipeline.png){style="display: block; margin:auto;"}

::: {.fragment}
> A stage outage stops the pipeline unless there’s redundancy.
:::

::: notes
Pipeline Parallelism is also used when a model is too large for a single GPU. Instead of splitting individual layers as in tensor parallelism, it works by dividing the model into sequential stages. Each GPU is responsible for one stage of the model, processing the output from the previous GPU and passing it to the next one in the pipeline. This is like an assembly line in a factory. A single car goes through different stations, and each station performs a different task. This allows you to chain multiple GPUs together to run an extremely large model. The challenge is that a stage outage can stop the entire pipeline unless there is redundancy.
:::

## Expert Parallelism for Mixture of Experts 

![Hybrid Parallelism Diagram original font:[https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism](https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism)](https://bentoml.com/llm/assets/images/ep-inference-8a8197f4144207ffddb62e81a4704a41.png)


:::notes
We've looked at the general-purpose parallelism techniques for large models. In an MoE model, only a small, specific subset of the model's 'experts' are activated for each input. Expert parallelism takes advantage of this by strategically splitting the experts themselves across different devices. As you can see here, each GPU holds only a subset of experts, rather than a full copy of the entire network. When a token needs to be processed, a router sends it to the specific GPU where its expert's full weights are stored. This approach is highly effective because it is memory-efficient. It avoids the memory overhead of replicating the full expert network on every device. This is what allows us to train and run much larger MoE models that would otherwise be impossible to fit on a single machine or even a standard cluster.
:::

## Hybrid Parallelism: Combining Techniques

::: {.fragment}
![Hybrid Parallelism Diagram original font:[https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism](https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism)](https://bentoml.com/llm/assets/images/dptp-e941197be9b1764dba8d90d2feb20069.png)
:::

::: {.fragment}
*Megatron-LM* combined *tensor* and *pipeline* parallelism to train a model with over **8.3 billion parameters**.
::: 



::: notes
In practice, a single strategy is often not enough. To train models with trillions of parameters, we have to use hybrid parallelism. A typical hybrid setup, like the one shown here, combines Data Parallelism and Tensor Parallelism. We use Data Parallelism by replicating the model across different groups of GPUs. Then, within each of those groups, we use Tensor Parallelism to split the model's layers across the GPUs, allowing the model to fit into memory. If we have eight GPUs, we could apply tensor parallelism across the first four GPUs, and then replicate that entire setup across the remaining four using data parallelism. This allows us to both increase the total training speed and run models that are far larger than any single GPU could handle. One of the first models to successfully demonstrate this on a massive scale was Megatron-LM, developed by NVIDIA.
:::

## AI-Era Challenges: More Complexity


- **Elasticity**: How do we dynamically scale a multi-GPU parallel training job without restarting it?

- **Redeployment**: How do we live-migrate a pipeline or a tensor-parallel workload across GPUs to balance load?

- **Scheduling**: How do we allocate not just a single GPU, but an entire group of interconnected GPUs with specific communication requirements?

- **Failure Management**: How do we recover from a single GPU failure in a tensor-parallel pipeline without bringing down the entire system?

::: notes
These new types of parallel jobs—data makes our core data center challenges even harder to solve. $\downarrow$ Take Elasticity. In the old cloud era, we would simply spin up a new virtual machine. But how do we dynamically scale a massive, multi-GPU training job without having to restart it from scratch? $\downarrow$ Similarly, Redeployment is no longer just about moving a web server. We now have to figure out how to live-migrate a complex pipeline or a tensor-parallel workload across GPUs to balance the load. $\downarrow$ And we aren't just allocating a single GPU. We have to allocate an entire group of interconnected GPUs that have specific, high-bandwidth communication requirements.$\downarrow$ And finally, If a single GPU fails in the middle of a tensor-parallel pipeline, how do we recover without bringing down the entire system?
:::

## The need of the Edge 

::: {style="text-align: center;"}
![](./figures/cloud-continuum.png){width=50%}
:::

::: notes
We've talked about the big, centralized data centers. But to truly deliver on the promises of AI, we need to go beyond that. This brings us to the idea of the Edge. The main cloud is great for training AI models. But for using them it's not always the best. We need to bring the computer power closer to the user. This diagram helps us understand why. At the top, we have the Cloud. This is where we have huge power and storage, but also higher latency because it's farther away. This is where we train models. As we move down to the Fog and then the Edge, we get closer to the user and the data source. At the Edge, we have devices like sensors and phones. This is where we get the benefit of lower latency and real-time processing. The key point is that it's not about choosing just one. It's about a connected and coordinated system that includes all three—from the large data centers in the cloud to the small devices at the edge.
:::


## The Multicloud Challenge {.smaller}

![](./figures/multicloud.png){style="display: block;margin-left: auto;margin-right: auto;"}


::: notes
We've talked about a single cloud. But today, many companies use multiple clouds. They use services from different providers at the same time. This multicloud approach makes our problem even more difficult. The system now has to make decisions across a whole network of clouds, not just one. We have to make sure our applications can run on any cloud without being locked in. We need to constantly find the cheapest cloud to run our tasks, without hurting performance. It's hard to keep security rules the same across different clouds. Services in different clouds need to talk to each other, and so on...
:::

## Challenges & Opportunities

- Evolve existing frameworks to support distributed AI training and inference across the entire cloud continuum.
- Efficient systems for right-sizing, rebalancing, and scheduling diverse workloads.
- Resilient to failure at every level, from a single rack in the data center to a disconnected device on the edge.

::: {.fragment}
> Infrastructure to support the next-generation of workloads, in the massive scale and also in the local and regional scale of fog and edge.
:::


::: notes
Just to conclude the message is clear: the challenges we face today require newer approaches. We need to build a new computing infrastructure that is smart, adaptable, and resilient. This is a system that spans the entire continuum, from the data center to the user. $\downarrow$ We can't just put AI on top of old systems; we need new frameworks designed to handle these distributed AI workloads across the entire cloud-to-edge continuum. $\downarrow$ Second, we need systems that can automatically right-size, rebalance, and schedule diverse workloads in real-time. $\downarrow$ And finally, we have to build resilience into every layer—from a single rack in the data center to a disconnected device on the edge—so the system can handle failures without disrupting service. $\downarrow$ Take home message, this is about building a coordinated infrastructure that supports the next generation of workloads, whether they are on a massive scale in the cloud or a local scale on the edge.
:::


## References {.smaller}